{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "9fba682fc10be871bef8d3585da705e4eba939aa8876addf909bc7b891431c82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "import torch\n",
    "import numpy as np\n",
    "import mmcv, cv2\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython import display\n",
    "\n",
    "from utils import *\n",
    "import skin_seg as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ConvAutoEncoderArrayDecoder(\n",
       "  (encoder): ConvAutoEncoder(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (batchNorm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchNorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchNorm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchNorm4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchNorm5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (decoders): ModuleList(\n",
       "    (0): ConvAutoDecoder(\n",
       "      (t_conv1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (t_conv2): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (t_conv3): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (t_conv4): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (t_conv5): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (t_batchNorm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (t_batchNorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (t_batchNorm3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (t_batchNorm4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "model = ss.ConvAutoEncoderArrayDecoder(num_decoders=1)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"models/skin_seg\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Face not visible!\n",
      "Face not visible!\n",
      "Face not visible!\n",
      "Face not visible!\n",
      "Face not visible!\n",
      "Face not visible!\n",
      "Face not visible!\n",
      "Face not visible!\n"
     ]
    }
   ],
   "source": [
    "# video = mmcv.VideoReader('video.mp4')\n",
    "# frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video]\n",
    "\n",
    "# display.Video('video.mp4', width=640)\n",
    "\n",
    "# define a video capture object\n",
    "vid = cv2.VideoCapture(0)\n",
    "face_not_visible = True\n",
    "  \n",
    "while(True):\n",
    "      \n",
    "    # Capture the video frame by frame\n",
    "    ret, frame = vid.read()\n",
    "\n",
    "    # Detect faces\n",
    "    frame = Image.fromarray(frame)\n",
    "    boxes, probs, landmarks = mtcnn.detect(frame, landmarks=True)\n",
    "\n",
    "    # Draw faces\n",
    "    frame_draw = frame.copy()\n",
    "    draw = ImageDraw.Draw(frame_draw)\n",
    "    #Check if net found faces\n",
    "    if(boxes is not None):\n",
    "        face_not_visible = False\n",
    "        for box, landmark in zip(boxes, landmarks):\n",
    "            draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "            draw.polygon(landmarks, outline=(255,0,0))\n",
    "            # print(landmark.shape)\n",
    "            # print(landmark)\n",
    "    else:\n",
    "        face_not_visible = True\n",
    "        print(\"Face not visible!\")\n",
    "\n",
    "    idx = 0\n",
    "    if(boxes is not None):\n",
    "        np_frame = np.asarray(frame)\n",
    "\n",
    "        area, idx = calc_area(boxes) # return area and idx of largest box (small filter if there are multiple people)\n",
    "        box = boxes[idx]\n",
    "\n",
    "        # calculate center of largest box and get bounding box\n",
    "        squares = calc_square_box(box, img_dim = np_frame.shape)\n",
    "\n",
    "        # extract \n",
    "        square_seg = np_frame[squares[3]:squares[1], squares[2]:squares[0]]\n",
    "        square_frame_original = cv2.resize(square_seg, (64,64), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        #Scaling for 64x64 model input\n",
    "        square_frame = np.transpose(square_frame_original, (2,0,1)) / 256\n",
    "        square_frame = np.reshape(square_frame, newshape=(1,) + square_frame.shape)\n",
    "        square_img = torch.tensor(square_frame, dtype=torch.float32)\n",
    "        square_img = square_img.to(device)\n",
    "\n",
    "        #Get Mask\n",
    "        masks, _ = model(square_img)\n",
    "        \n",
    "        masks = masks[0].cpu().detach().numpy()\n",
    "        mask = np.transpose(masks[0], (1,2,0))\n",
    "        mask_reshape = cv2.resize(mask, (square_seg.shape[1], square_seg.shape[0]))\n",
    "        mask = mask_reshape > 0.6 #mask\n",
    "        mask = np.reshape(mask, mask.shape + (1,))\n",
    "        img_masked = square_seg * mask #mask overlayed on image\n",
    "\n",
    "        cv2.imshow('img_crop', square_seg)\n",
    "        cv2.imshow('mask', mask_reshape)\n",
    "        cv2.imshow('mask on image', img_masked)\n",
    "\n",
    "        draw.rectangle(squares.tolist(), outline=(0, 255, 0), width=6)\n",
    "\n",
    "\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame_draw', np.asarray(frame_draw))\n",
    "    # the 'q' button is set as quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "  \n",
    "# After the loop release the cap object\n",
    "vid.release()\n",
    "# Destroy all the windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the loop release the cap object\n",
    "vid.release()\n",
    "# Destroy all the windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "squares[0]\n",
    "# np_fram[int(squares[0]):int(squares[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}